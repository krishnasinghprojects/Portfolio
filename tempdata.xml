
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html" version="2.0">
<channel>
<title>
<![CDATA[ Stories by Krishna Singh on Medium ]]>
</title>
<description>
<![CDATA[ Stories by Krishna Singh on Medium ]]>
</description>
<link>https://medium.com/@krishnasinghprojects?source=rss-12abf4f5923a------2</link>
<image>
<url>https://cdn-images-1.medium.com/fit/c/150/150/0*wkwdqr86wBOv0-3R</url>
<title>Stories by Krishna Singh on Medium</title>
<link>https://medium.com/@krishnasinghprojects?source=rss-12abf4f5923a------2</link>
</image>
<generator>Medium</generator>
<lastBuildDate>Wed, 22 Oct 2025 22:55:39 GMT</lastBuildDate>
<atom:link href="https://medium.com/@krishnasinghprojects/feed" rel="self" type="application/rss+xml"/>
<webMaster>
<![CDATA[ yourfriends@medium.com ]]>
</webMaster>
<atom:link href="http://medium.superfeedr.com" rel="hub"/>
<item>
<title>
<![CDATA[ How to Run a Local LLM with Ollama and Node JS for Beginners ]]>
</title>
<link>https://medium.com/@krishnasinghprojects/how-to-run-a-local-llm-with-ollama-and-node-js-for-beginners-a4d13e49737c?source=rss-12abf4f5923a------2</link>
<guid isPermaLink="false">https://medium.com/p/a4d13e49737c</guid>
<category>
<![CDATA[ programming ]]>
</category>
<category>
<![CDATA[ artificial-intelligence ]]>
</category>
<category>
<![CDATA[ nodejs ]]>
</category>
<category>
<![CDATA[ javascript ]]>
</category>
<category>
<![CDATA[ ollama ]]>
</category>
<dc:creator>
<![CDATA[ Krishna Singh ]]>
</dc:creator>
<pubDate>Wed, 22 Oct 2025 22:26:48 GMT</pubDate>
<atom:updated>2025-10-22T22:36:49.009Z</atom:updated>
<content:encoded>
<![CDATA[ <h4>Ditch the APIs. This step-by-step guide for a complete beginner to get started with a powerful, private, and free-to-run AI on own local machine.</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*U8YuAbz3rXnfEHU3rlPfXA.png" /><figcaption>INTRODUCTION</figcaption></figure><p>Ever wanted your own private ChatGPT? This tutorial provides everything you need to install Ollama, download a language model, and write a simple NodeJS script to begin interacting with a powerful AI running entirely on your computer.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*P2csscJbeNfH8uI73xeS6Q.png" /><figcaption>gpt-oss:20b — Locally Running LLM</figcaption></figure><h3><strong>What You’ll Achieve in This Tutorial</strong></h3><p>By the end of this post, you will have:</p><ul><li>A powerful AI language model running <strong>entirely on your own computer</strong>, completely private and offline.</li><li>The ability to chat with your AI directly through the <strong>command line</strong> for quick questions and tests.</li><li>A ready-to-use <strong>NodeJS script</strong> that gives you programmatic control over your AI, serving as a foundation for future projects.</li></ul><p>So without further ado, Let’s dive right into the tutorial.</p><h3>Step 1 : Installing Ollama</h3><p><strong>Ollama</strong> is an open-source tool that simplifies running large language models (LLMs) like Llama 3 and Codestral directly on your local machine, rather than through a cloud service.</p><p>First, head over to the <a href="https://ollama.com/download"><strong>Ollama.com</strong></a><strong> </strong>and download the installer for your operating system (macOS, Linux, or Windows).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ogZalLJi7XYRXHR9LT0wYQ.png" /><figcaption>Ollama Download Page</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*hnyxXmkYtr2tCVB5tCe-rA.png" /><figcaption>MacOS and Windows Installer</figcaption></figure><p>Run the installer and follow the on-screen prompts. It’s a straightforward installation, and once it’s finished, Ollama will be running quietly in the background.</p><h3><strong>Step 2 : </strong>Verify Installation &amp; Understand System Needs</h3><p>Now that Ollama is installed, let’s make sure it’s working and download our first AI model.</p><blockquote>Firstly Verify the Installation</blockquote><p>Open your terminal (Terminal on Mac/Linux, PowerShell or Command Prompt on Windows) and type the following command:</p><pre>ollama --version</pre><p>If it’s installed correctly, you’ll see the version number printed out.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*oMCVSHwQiBSGRccVsp7nGA.png" /><figcaption>Command To Check Ollama Version</figcaption></figure><blockquote><strong>Quick Tip:</strong> If you get an error like “command not found,” first try closing and reopening your terminal. If that doesn’t work, a quick restart of your computer almost always fixes it. If it still doesn’t show up, Ollama may not have installed correctly.</blockquote><p>SYSTEM REQUIREMENTS — <strong>IMPORTANT</strong></p><p>Running an AI on your own machine might sound intense, but you don’t need a supercomputer to start ! The requirements depend entirely on the <strong>size</strong> of the model you want to run.</p><h4>General Rule of Thumb</h4><ul><li><strong>RAM (System Memory):</strong> You need at least as much free RAM as the size of the model (Required to load modal in the memory). For a smooth experience, <strong>8 GB of RAM</strong> is a good minimum for smaller models, with <strong>16 GB</strong> being ideal.</li><li><strong>VRAM (Graphics Card Memory):</strong> This is for speed. If you have a dedicated graphics card (like an NVIDIA or AMD GPU) with enough VRAM, the AI will run much faster. If not, Ollama will automatically use your CPU, which is slower but still works perfectly fine !</li></ul><h4>Requirements for Phi3 (Model We Will Be Using)</h4><p>The phi3 model we will be using is small and efficient, making it perfect for most modern computers.</p><ul><li><strong>Model Size:</strong> ~2.3 GB</li><li><strong>Required RAM:</strong> At least <strong>8 GB</strong> of system RAM.</li><li><strong>Recommended VRAM:</strong> A graphics card with <strong>4 GB</strong> or more of VRAM.</li><li><strong>Performance (Tokens/Sec):</strong> On a modern CPU, you can expect a usable 5–10 tokens/sec. With a GPU like an <strong>NVIDIA RTX 3050</strong>, that jumps to a very fast <strong>20–35+ tokens/sec</strong>.</li></ul><blockquote>If you plan to try other models (like llama3 or mistral), remember to check their requirements first. A larger model will need significantly more RAM and VRAM. You can find the size for any model on the <a href="https://ollama.com/library"><strong>Ollama Library page</strong></a> before you download it.</blockquote><h3>Step 3 : Download &amp; Chat with Your First Model</h3><p>Now this is where the fun begins !</p><p>Since now we have installed Ollama, we’ll tell Ollama to download a model. We’ll use phi3, a great small model from Microsoft that is fast and capable.</p><p>Run this command in your terminal :</p><pre>ollama pull phi3</pre><p>You’ll see a download progress bar. This might take a few minutes depending on your internet connection.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*t-uXQK6X8a2BOKz77qNmVw.png" /><figcaption>phi3 Download</figcaption></figure><h4>Chat with Your Model!</h4><p>Once the download is complete, you can immediately start chatting with your AI directly in the terminal using this command:</p><pre>ollama run phi3</pre><p>Your terminal will turn into a chat window. Go ahead, ask it a question! To exit, just type /bye. This is a great way to confirm everything is working before we even start coding.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ZmGMruCREcffFTt6nOpP-g.png" /><figcaption>phi3 — Locally Running LLM</figcaption></figure><h3>Step 4 : Control the AI with NodeJS</h3><p>Now for the advanced code part — Writing a program to interact with our model.</p><p>Prerequisite: <strong>NodeJS </strong>Installed.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*vGT30mxnGEDp9EdsKJ2IkA.png" /><figcaption>Node JS Official Webpage</figcaption></figure><blockquote>You’ll need <strong>NodeJS</strong> installed on your computer. If you don’t have it, you can download it from the <a href="https://nodejs.org/en"><strong>NodeJS</strong></a>.</blockquote><h4><strong>1. Project Setup</strong></h4><ul><li>Create a new folder for your project and open it in your code editor (like VS Code).</li><li>Inside that folder, create a new file named index.js.</li><li>Open the integrated terminal in your editor and run the following command to install the official Ollama library :</li></ul><pre>npm install ollama</pre><p>This command creates a node_modules folder for the library. Now you&#39;re ready to write the script in your index.js file.</p><h4><strong>2. Writing The Script</strong></h4><p>Open your index.js file. We&#39;ll add the code piece by piece explaining every snippet.</p><ul><li><strong>Import the Ollama Library</strong> First, we need to import the tools from the ollama library we just installed. This line above gives us access to the <strong>Ollama SDK</strong>.</li></ul><pre>import { Ollama } from &#39;ollama&#39;;</pre><blockquote><strong><em>Note:</em></strong><em> An SDK (Software Development Kit) is a toolkit provided by developers to make it easy to use their application’s features in your own code.</em></blockquote><ul><li><strong>Establish the Connection </strong>Next, we’ll create a constant that connects to the Ollama service running on your machine.</li></ul><pre>const ollama = new Ollama({ host: &#39;http://localhost:11434&#39; });</pre><ul><li><strong>Create the Main Function &amp; Your Prompt</strong> We’ll wrap our logic in an async function. This allows us to use the await keyword to wait for the AI&#39;s response. Inside, we&#39;ll define our question.</li></ul><blockquote>We use <em>async</em> and <em>await</em> because it&#39;s a modern JavaScript best practice. It lets our script ask the AI a question and then patiently wait for the answer <strong>without freezing the entire program</strong>. This is known as &quot;non-blocking&quot; code and is essential for building fast, responsive applications.</blockquote><pre>async function main() {<br> // Define the question we want to ask the AI<br> const userPrompt = &quot;Why is life ?&quot;;<br> console.log(`\nYour Question: ${userPrompt}`);<br> // The rest of our code will go here...<br>}</pre><p>We store our question in a variable called userPrompt.</p><blockquote>Storing our question in the userPrompt variable, instead of writing it directly inside the final function call, is a great habit. It allows us to write <strong>clean, readable code</strong> and makes it easy to change the question later without digging through other logic.</blockquote><ul><li><strong>Send the Prompt to the AI</strong> Now, let’s add the code inside our main function that sends the prompt to the model.</li></ul><pre>// Send the prompt to the AI model and wait for the answer<br> const response = await ollama.chat({<br> model: &#39;phi3&#39;,<br> messages: [{ role: &#39;user&#39;, content: userPrompt }],<br> });</pre><p>This is the core of our script. We call the chat function, tell it to use the phi3 model, and pass it our userPrompt.</p><ul><li><strong>Get the Answer and Run the Script</strong> The AI’s response comes back as an object. We just need to grab the text content and print it. Finally, we call our main function to make everything run.</li></ul><pre>// Store the AI&#39;s answer in a variable<br> const aiAnswer = response.message.content;<br> console.log(`\nAI&#39;s Answer: ${aiAnswer}`);<br>}<br>// Run the main function<br>main();</pre><p>And that’s it! Your file now contains the complete program, broken down into understandable steps.</p><h4>3. Running The Script</h4><p>To run your script, save the file and type this command in your terminal :</p><pre>node index.js</pre><blockquote><strong>NOTE :</strong> If you see a warning about &quot;MODULE_TYPELESS_PACKAGE_JSON&quot; when running node index.js, Don&#39;t worry! It just means NodeJS wants you to explicitly declare your script as a &quot;module.&quot;</blockquote><blockquote><strong>Quick Fix :</strong> Either rename your file to index.mjs (and run node index.mjs), or modify the package.json file in your project folder add a line <em>&quot;type&quot;: &quot;module&quot;</em>below dependencies inside it. The code will work fine either way.</blockquote><p>You should see your question and the AI’s answer printed out right there. Congratulations, you’ve successfully controlled a local LLM with code !</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Aqu1itKY-E1a1hFT2Exb6w.png" /><figcaption>phi3 — Running in NodeJS</figcaption></figure><blockquote>Complete Source Code</blockquote><pre>import { Ollama } from &#39;ollama&#39;;<br>// Establish a connection to Ollama<br>const ollama = new Ollama({ host: &#39;http://localhost:11434&#39; });<br>// This is where our main logic will go<br>async function main() <br>{<br> console.log(&quot;Starting our AI interaction...&quot;);<br>// 1. Define the question we want to ask the AI<br> const userPrompt = &quot;Why is Life?&quot;;<br> console.log(`\nYour Question: ${userPrompt}`);<br>// 2. Send the prompt to the AI model and wait for the answer<br> const response = await ollama.chat({<br> model: &#39;phi3&#39;,<br> messages: [{ role: &#39;user&#39;, content: userPrompt }],<br> });<br>// 3. Store the AI&#39;s answer in a variable<br> const aiAnswer = response.message.content;<br> console.log(`\nAI&#39;s Answer: ${aiAnswer}`);<br>}<br>// Run the main function<br>main();</pre><blockquote>Congratulations ! You’ve successfully installed Ollama, run your first AI model, and even controlled it with NodeJS. You now have the foundation for building almost any AI-powered application, all running privately on your own machine.</blockquote><p><em>I hope this guide was helpful. Feel free to leave a comment if you have any questions !</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=a4d13e49737c" width="1" height="1" alt=""> ]]>
</content:encoded>
</item>
<item>
<title>
<![CDATA[ I Built an AI-Powered Random Web Page Generator — All in One HTML File ]]>
</title>
<link>https://medium.com/@krishnasinghprojects/i-built-an-ai-powered-random-web-page-generator-all-in-one-html-file-812ed4a78582?source=rss-12abf4f5923a------2</link>
<guid isPermaLink="false">https://medium.com/p/812ed4a78582</guid>
<category>
<![CDATA[ artificial-intelligence ]]>
</category>
<category>
<![CDATA[ javascript ]]>
</category>
<category>
<![CDATA[ web-development ]]>
</category>
<category>
<![CDATA[ generative-ai-use-cases ]]>
</category>
<category>
<![CDATA[ programming ]]>
</category>
<dc:creator>
<![CDATA[ Krishna Singh ]]>
</dc:creator>
<pubDate>Tue, 21 Oct 2025 05:46:18 GMT</pubDate>
<atom:updated>2025-10-21T05:46:18.865Z</atom:updated>
<content:encoded>
<![CDATA[ <h3><strong>I Built an AI-Powered Random Web Page Generator — All in One HTML File</strong></h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*VJabvy6pIXPlIuFA0w4Ndw.png" /><figcaption>Generated Random Webpage</figcaption></figure><p>Ever thought of what it would be like to refresh a webpage and see something completely new every single time ?</p><p>I built a web page that does exactly that. One moment it’s generative art, the next a mini-game, and then a quirky utility — all powered by AI and delivered in <strong>a single HTML file</strong>.</p><p>No backend, no frameworks, just pure JS and clever API calls.</p><p>(API Key is Hard Coded It’s Just A Fun Project)</p><p><em>In this post, I’ll break down how I made the AI generate entire webpages on the fly, how the loader and iframe magic work, and what I learned along the way.</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*uoN2M-doG527QpjQ5zDyeg.png" /><figcaption>Another Random Generated Webpage</figcaption></figure><blockquote><strong>So How And Why Did I Do It ?</strong></blockquote><p>We’ve all seen AI generate content or images. But why not a full webpage? <strong>HTML, CSS, JavaScript</strong> — all created on the fly, delivered directly to the user.</p><p>This approach keeps things simple. No need to store 1,000 variations. Just one shell, and a single API call.</p><h3>1. The Stage (HTML Shell)</h3><p>First, I needed a “stage” for the AI-generated content. The HTML is surprisingly simple:</p><p><strong>Loading screen (</strong><strong>&lt;div id=&quot;loader&quot;&gt;):</strong> Shows while the AI builds the page. (Just A Gimmick Loading Bar Doesn’t Work).</p><p><strong>Empty iframe (</strong><strong>&lt;iframe id=&quot;content-frame&quot;&gt;):</strong> Invisible container where the AI’s HTML will appear.</p><p>That’s It Just These Two Things In HTML and We’re Done.</p><p><strong>(NOTE :</strong> I Have Skipped CSS Code As Its Just Designing And I’m Here To Explain The Logical Aspect Of The Mini-Project)</p><pre><br>&lt;body&gt;<br> &lt;div id=&quot;loader&quot;&gt;<br> &lt;div class=&quot;loading-content&quot;&gt;<br> &lt;div class=&quot;loading-text&quot;&gt;Loading&lt;/div&gt;<br> &lt;div class=&quot;progress-bar-container&quot;&gt;<br> &lt;div class=&quot;progress-bar&quot;&gt;&lt;/div&gt;<br> &lt;/div&gt;<br> &lt;/div&gt;<br> &lt;div id=&quot;error-message&quot;&gt;&lt;/div&gt;<br> &lt;/div&gt;<br>&lt;iframe id=&quot;content-frame&quot; title=&quot;Generated Content&quot;&gt;&lt;/iframe&gt;<br>&lt;script&gt;<br> // JS Logic goes here<br> &lt;/script&gt;<br>&lt;/body&gt;</pre><p><em>[CSS hides the iframe and styles the loader as a full-screen overlay]</em></p><h3>2. The Engine (JavaScript)</h3><p>All the magic happens in a single &lt;script&gt; tag.</p><blockquote>Step 1 : API KEY &amp; DOM Elements</blockquote><pre>document.addEventListener(&#39;DOMContentLoaded&#39;, () =&gt; {<br> const YOUR_API_KEY = &quot;API_KEY_HERE&quot;;<br> const loader = document.getElementById(&#39;loader&#39;);<br> const loadingContent = document.querySelector(&#39;.loading-content&#39;);<br> const contentFrame = document.getElementById(&#39;content-frame&#39;);<br> const generalErrorMessage = document.getElementById(&#39;error-message&#39;);<br> const progressBar = document.querySelector(&#39;.progress-bar&#39;);</pre><p>In this step, we get the reference to the DOM elements by their id as well as initialize the API Key.</p><p>(For this project I have used Gemini API Key as its free and provide gemini-2.5-flash model with good rate limits. To get an API Key for yourself go to <a href="https://aistudio.google.com/api-keys">Google AI Studio</a>).</p><blockquote>Step 2 : The Prompt</blockquote><pre>const PROMPT = `<br> You are a visionary web developer...<br> Generate a unique, self-contained HTML file...<br> VARIETY MANDATE: choose ONE random category: 🎨 Generative Art, 🛠️ Utility Tools, 🎮 Mini Games...<br> Output: ONLY raw HTML starting with &lt;!DOCTYPE html&gt;<br> `;</pre><p>In This Step, We write a clear prompt to generate a random HTML Webpage code.</p><p>(<strong>NOTE : </strong>It’s an oversimplified Prompt, for it to generate better results you should write a better clear prompt with all details)</p><blockquote>Step 3: Trigger &amp; Validation</blockquote><pre>if (!YOUR_API_KEY) {<br> loadingContent.style.display = &#39;none&#39;;<br> generalErrorMessage.textContent = &#39;Error: Please add your Gemini API key.&#39;;<br> generalErrorMessage.style.display = &#39;block&#39;;<br> } else {<br> generatePage(YOUR_API_KEY);<br> }</pre><p>In this step, we check if API Key Exists or not. If it doesn’t exist we show an error else we move forward with the <em>generatePage</em> function.</p><blockquote>Step 4: Prepare And Send API Request</blockquote><pre>async function generatePage(apiKey) {<br> const GEMINI_API_URL = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key=${apiKey}`;<br> try {<br> const payload = { contents: [{ parts: [{ text: PROMPT.trim() }] }] };<br> const response = await fetch(GEMINI_API_URL, {<br> method: &#39;POST&#39;,<br> headers: { &#39;Content-Type&#39;: &#39;application/json&#39; },<br> body: JSON.stringify(payload)<br> });</pre><p>In this step,</p><p>First we prepare the URL to send an API Request, We inject the API Key in the URL as well as the model name.</p><p>Then for the payload we insert our prompt in the contents.</p><p>Lastly, we send a fetch POST request to the URL and store its response.</p><p>(<strong>NOTE : </strong>It’s <em>asynchronous </em>call so we use async/await function. DON’T use normal functions)</p><blockquote>Step 5: Response Cleaning</blockquote><pre>if (!response.ok) throw new Error(`HTTP error! Status: ${response.status}`);<br> const data = await response.json(); <br>let htmlContent = data.candidates?.[0]?.content?.parts?.[0]?.text;<br> if (!htmlContent) throw new Error(&quot;Empty response from AI.&quot;);<br> htmlContent = htmlContent.replace(/^```html\s*|```$/g, &#39;&#39;).trim();<br></pre><p>In this step, we check wether we got a response or not and if so, we parse the response into clear HTML syntax so that it can be rendered properly else we throw an error.</p><h3>Part 3: The Reveal</h3><pre>contentFrame.srcdoc = htmlContent;<br> progressBar.style.animation = &#39;none&#39;;<br> progressBar.style.transform = &#39;scaleX(1)&#39;;<br> contentFrame.onload = () =&gt; {<br> setTimeout(() =&gt; {<br> loader.classList.add(&#39;fade-out&#39;);<br> contentFrame.classList.add(&#39;visible&#39;);<br> document.body.style.overflow = &#39;auto&#39;;<br> }, 100);<br> };<br> } catch (error) {<br> console.error(&#39;Failed to generate page:&#39;, error);<br> loadingContent.style.display = &#39;none&#39;;<br> generalErrorMessage.textContent = `Error: ${error.message}`;<br> generalErrorMessage.style.display = &#39;block&#39;;<br> }<br> }<br>});</pre><p>Once the AI responds, the iframe renders a <em>full, interactive webpage</em>.</p><p>In this code snippet, we enter the cleaned HTML file into the iframe and set it to visible with proper transition.</p><p>And That’s it we just created entire Random Webpage Generator.</p><p>Refresh, and you get something completely new every time.</p><p>It’s a fun, weird, and creative way to think about web development — all from a single HTML file and a single API call.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*MkGFNRxuLcJNCdv3EpxCCg.png" /><figcaption>Random Generated Webpage</figcaption></figure><blockquote>External Links (Source Code/Live Demo)</blockquote><p><a href="https://random.krishnasingh.live/">Random - Discover Unique Web Experiences</a></p><p><strong>NOTE : </strong>You need API Key to try it. Generate from here <a href="https://aistudio.google.com/api-keys">Google AI Studio</a>. It only gets stored in <em>localStorage </em>only.</p><p><a href="https://github.com/krishnasinghprojects/Random">GitHub - krishnasinghprojects/Random</a></p><p><strong>NOTE : </strong>This is an advanced version of the project it has few things like API validation, API Storing and Loading. The core logic is still the same.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=812ed4a78582" width="1" height="1" alt=""> ]]>
</content:encoded>
</item>
</channel>
</rss>